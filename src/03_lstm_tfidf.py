# -*- coding: utf-8 -*-
"""03_LSTM_TFIDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jpbJVoeXXWGj3xFl1E8-rFqBxaNlZNL9

# 2025 COMP90042 Project - Group 43

## Object-Oriented Programming (OOP) Components

This section defines the core classification pipeline using object-oriented programming principles to promote **modularity**, **readability**, and **reuse**.

### Implemented Classes:
- `LSTMDatasetCached`: A PyTorch `Dataset` for loading claim-evidence pairs with cached retrieval.

- `LSTMWithAttention`：sequence classifier with attention mechanism

These components are designed to be easily integrated into training and evaluation loops.
"""

import re
import torch
from torch.utils.data import Dataset

def tokenize(text):
    return re.findall(r'\w+', text.lower())

class LSTMDatasetCached(Dataset):
    def __init__(self, claims_data, evidence_dict, retrieved_evidences, vocab, max_len=256):
        self.samples = list(claims_data.items())
        self.evidence_dict = evidence_dict
        self.retrieved_evidences = retrieved_evidences  # {'cid': {'claim_text': ..., 'evidence_ids': [...]}}
        self.vocab = vocab
        self.max_len = max_len
        self.label2id = {"SUPPORTS": 0, "REFUTES": 1, "NOT_ENOUGH_INFO": 2, "DISPUTED": 3}

    def __len__(self):
        return len(self.samples)

    def encode(self, text):
        tokens = tokenize(text)
        token_ids = [self.vocab.get(t, self.vocab['<UNK>']) for t in tokens]
        if len(token_ids) < self.max_len:
            token_ids += [self.vocab['<PAD>']] * (self.max_len - len(token_ids))
        else:
            token_ids = token_ids[:self.max_len]
        return torch.tensor(token_ids)

    def __getitem__(self, idx):
        claim_id, info = self.samples[idx]
        claim = info['claim_text']
        label = self.label2id[info['claim_label']]

        evidence_ids = self.retrieved_evidences[claim_id]['evidence_ids']
        evidence_texts = [self.evidence_dict.get(eid, '') for eid in evidence_ids]
        input_text = claim + ' ' + ' '.join(evidence_texts)
        input_ids = self.encode(input_text)

        return {
            "claim_id": claim_id,
            "input_ids": input_ids,
            "label": torch.tensor(label, dtype=torch.long)
        }

import torch.nn as nn

class LSTMWithAttention(nn.Module):
    def __init__(self, vocab_size, embed_size=100, hidden_size=128, num_classes=4, embedding_weights=None):
        super(LSTMWithAttention, self).__init__()

        if embedding_weights is not None:
            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=False)
        else:
            self.embedding = nn.Embedding(vocab_size, embed_size)

        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)
        self.attention = nn.Linear(hidden_size * 2, 1)  # attention score per time step
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, input_ids):
        embedded = self.embedding(input_ids)           # [B, T, E]
        outputs, _ = self.lstm(embedded)               # [B, T, 2H]

        attn_scores = self.attention(outputs)          # [B, T, 1]
        attn_weights = torch.softmax(attn_scores, dim=1) # [B, T, 1]
        context = (outputs * attn_weights).sum(dim=1)  # [B, 2H]

        context = self.dropout(context)
        logits = self.classifier(context)              # [B, num_classes]
        return logits

"""# 1.DataSet Processing

## Setup: Load Libraries and Datasets

This section loads all required Python libraries and the datasets needed for training and evaluation.

### Steps:
- Imports necessary libraries (`torch`, `transformers`, `sklearn`, etc.)
- Loads the JSON datasets:
  - `train-claims.json`
  - `dev-claims.json`
  - `evidence.json`
- Initializes tokenizer and random seed
- Applies stratified splitting on training claims
- Deduplicates the evidence corpus based on normalized MD5 hashes
"""

!pip install sentence-transformers

"""# Setup: Load Libraries and Datasets

This section loads all required Python libraries and the provided datasets.

- Loads `train-claims.json`, `dev-claims.json`, and `evidence.json`
- Imports key libraries: `torch`, `transformers`, `faiss`, `numpy`, etc.

"""

import json
import torch
import hashlib
from collections import defaultdict
from transformers import AutoTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np
import random
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Load the datasets
with open('train-claims.json', 'r') as f:
    train_claims = json.load(f)

with open('dev-claims.json', 'r') as f:
    dev_claims = json.load(f)

with open('evidence.json', 'r') as f:
    evidence_dict = json.load(f)

evidence_texts = list(evidence_dict.values())
evidence_ids = list(evidence_dict.keys())

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# random seed
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Prepare claim IDs and their labels
claim_ids = list(train_claims.keys())
labels = [train_claims[cid]["claim_label"] for cid in claim_ids]

# Stratified split (e.g., 80% train, 20% dev)
train_ids, dev_ids = train_test_split(
    claim_ids,
    test_size=0.2,
    stratify=labels,
    random_state=42
)

# Rebuild the dicts
train_claims_strat = {cid: train_claims[cid] for cid in train_ids}
dev_claims_strat   = {cid: train_claims[cid] for cid in dev_ids}

"""## Evidence Deduplication (by Text Hashing)

Normalizes and hashes each evidence text to identify exact duplicates.
Only one canonical version is kept for retrieval; duplicates are mapped for later expansion if needed.
"""

def normalize_text(text):
    return text.strip().lower()

def hash_text(text):
    return hashlib.md5(normalize_text(text).encode()).hexdigest()

# Deduplicate
hash_to_ids = defaultdict(list)
hash_to_text = {}

for evid_id, text in evidence_dict.items():
    h = hash_text(text)
    hash_to_ids[h].append(evid_id)
    if h not in hash_to_text:
        hash_to_text[h] = text

# Canonical evidence set for indexing
dedup_evidence_dict = {hash_to_ids[h][0]: hash_to_text[h] for h in hash_to_text}
evidence_ids = list(dedup_evidence_dict.keys())
evidence_texts = list(dedup_evidence_dict.values())

# Map from canonical ID to all original duplicates (for later output expansion)
canonical_to_all_ids = {hash_to_ids[h][0]: hash_to_ids[h] for h in hash_to_ids}

with open("canonical_to_all_ids.json", "w") as f:
    json.dump(canonical_to_all_ids, f)

"""## Setup for Each Retriever Methods  
- `retrieve_tfidf()`: Symbolic retrieval using TF-IDF similarity  

"""

# TF-IDF Retriever setup

# Fit TF-IDF vectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)
evidence_tfidf = vectorizer.fit_transform(evidence_texts)

# Define TF-IDF retrieval function
def retrieve_tfidf(claim_text, top_k=5):
    claim_vec = vectorizer.transform([claim_text])
    similarities = cosine_similarity(claim_vec, evidence_tfidf).flatten()
    top_k_indices = similarities.argsort()[-top_k:][::-1]
    return [evidence_ids[i] for i in top_k_indices]

"""## Gold Evidence Injection

This function ensures that gold evidence is present in the retrieved results (e.g., for supervised training). It merges gold IDs with retrieved IDs, preserving order and trimming to `top_k`.
"""

def inject_gold_evidence(train_claims, retrieved_dict, top_k=5):
    for cid, info in train_claims.items():
        gold_ids = info.get("evidences", [])
        current = retrieved_dict.get(cid, [])
        merged = list(dict.fromkeys(gold_ids + current))[:top_k]
        retrieved_dict[cid] = merged

# Constructing training set retrieval results + gold evidence
tfidf_train_retrieved = {
    cid: retrieve_tfidf(info["claim_text"], top_k=5)
    for cid, info in train_claims.items()
}

inject_gold_evidence(train_claims, tfidf_train_retrieved, top_k=5)

precomputed_train = {
    cid: {
        "claim_text": train_claims[cid]["claim_text"],
        "evidence_ids": tfidf_train_retrieved[cid]
    }
    for cid in train_claims
}

# Construct validation set retrieval results (no gold evidence)
precomputed_dev = {
    cid: {
        "claim_text": info["claim_text"],
        "evidence_ids": retrieve_tfidf(info["claim_text"], top_k=5)
    }
    for cid, info in dev_claims.items()
}

# save
with open("train_retrieved_checked.json", "w") as f:
    json.dump(precomputed_train, f)

with open("dev_retrieved_checked.json", "w") as f:
    json.dump(precomputed_dev, f)

"""# 2.Model Implementation
## LSTM Model

This section defines the LSTM-based text classification pipeline for claim verification. It includes:
- A vocabulary builder from retrieved evidence
- A standard training loop with early stopping and class weighting

### Vocabulary Construction

Builds a vocabulary from tokenized claim–evidence pairs in the training set.
- Uses top `max_vocab` most frequent words
- Reserves special tokens `<PAD>` and `<UNK>`
"""

from collections import Counter

with open("train_retrieved_checked.json") as f:
    precomputed_train = json.load(f)

with open("dev_retrieved_checked.json") as f:
    precomputed_dev = json.load(f)

def build_vocab_with_cache(claims_data, evidence_dict, retrieved_evidences, max_vocab=20000):
    counter = Counter()
    for cid, claim_info in claims_data.items():
        claim = claim_info['claim_text']
        evidence_ids = retrieved_evidences[cid]['evidence_ids']
        text = claim + ' ' + ' '.join([evidence_dict.get(eid, '') for eid in evidence_ids])
        counter.update(tokenize(text))
    most_common = counter.most_common(max_vocab - 2)
    vocab = {'<PAD>': 0, '<UNK>': 1}
    vocab.update({word: i + 2 for i, (word, _) in enumerate(most_common)})
    return vocab

"""### LSTM Training Function

Implements a standard supervised training loop with:
- Weighted cross-entropy loss
- Early stopping based on dev accuracy
- Model checkpoint saving
"""

from torch.utils.data import DataLoader

# vocab
vocab_tfidf = build_vocab_with_cache(train_claims, evidence_dict, precomputed_train)

# Dataset
train_dataset = LSTMDatasetCached(train_claims, evidence_dict, precomputed_train, vocab_tfidf)
dev_dataset = LSTMDatasetCached(dev_claims, evidence_dict, precomputed_dev, vocab_tfidf)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
dev_loader   = DataLoader(dev_dataset, batch_size=32)
dev_claim_ids = list(dev_claims.keys())

from sklearn.utils.class_weight import compute_class_weight

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# batch_size=8/16/32/64
# lr=1e-3/2e-4/5e-5
# epoch=3/5/10/20
# hidden_size=64/128/256/512
# embed_size=64/100/128/256

model = LSTMWithAttention(
    vocab_size=len(vocab_tfidf),
    embed_size=128,
    hidden_size=128,
    num_classes=4
).to(device)

optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,
    # weight_decay=1e-5 # L2 regularization
    )

# label distribution
label2id = {"SUPPORTS": 0, "REFUTES": 1, "NOT_ENOUGH_INFO": 2, "DISPUTED": 3}

all_labels = [label2id[claim["claim_label"]] for claim in train_claims.values()]

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.array([0, 1, 2, 3]),
    y=all_labels
)

class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
print("Class weights:", class_weights)

loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

def evaluate(model, dataloader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['label'].to(device)

            logits = model(input_ids)
            predictions = torch.argmax(logits, dim=1)
            correct += (predictions == labels).sum().item()
            total += labels.size(0)

    accuracy = correct / total * 100
    return accuracy

from tqdm import tqdm

# Early stopping parameters
num_epochs = 20
patience = 5
best_val_acc = 0
patience_counter = 0
train_losses = []
val_accuracies = []

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
        input_ids = batch['input_ids'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        logits = model(input_ids)
        loss = loss_fn(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    train_losses.append(avg_loss)

    val_acc = evaluate(model, dev_loader)
    print(f"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}")
    print(f"Epoch {epoch+1} - Val Accuracy: {val_acc:.2f}%")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        torch.save(model.state_dict(), "lstm_tfidf_best_model.pt")
        print("Model improved and saved.")
    else:
        patience_counter += 1
        print(f"No improvement. Patience: {patience_counter}/{patience}")
        if patience_counter >= patience:
            print("Early stopping triggered.")
            break

print(f"Best validation accuracy: {best_val_acc:.2f}%")

import matplotlib.pyplot as plt

plt.plot(train_losses, marker='o')
plt.title("Training Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""# 3.Testing and Evaluation
This section evaluates the LSTM classifier trained on hybrid-retrieved evidence. It includes:

- Classification performance on the dev set
- Training with class imbalance handling
- Prediction file generation with evidence ID expansion
"""

from sklearn.metrics import classification_report

def evaluate_model(model, dev_loader, device):
    model.to(device)
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in dev_loader:
            inputs = batch['input_ids'].to(device)
            labels = batch['label'].to(device)

            logits = model(inputs)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print(classification_report(
        all_labels, all_preds,
        target_names=["SUPPORTS", "REFUTES", "NOT_ENOUGH_INFO", "DISPUTED"]
    ))

evaluate_model(model, dev_loader, device)

"""## Prediction File Generation

Generates final predictions for the full `dev-claims.json` set and expands canonical evidence IDs back to their original form.
"""

# Expand function
def expand_ids(eids, mapping, top_k=5):
    expanded = []
    for eid in eids:
        expanded.extend(mapping.get(eid, [eid]))
    return list(dict.fromkeys(expanded))[:top_k]

import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import json

model = LSTMWithAttention(
    vocab_size=len(vocab_tfidf),
    embed_size=128,
    hidden_size=128,
    num_classes=4
)
model.load_state_dict(torch.load("lstm_tfidf_best_model.pt", map_location=device))
model.to(device)
model.eval()

with open("dev_retrieved_checked.json") as f:
    precomputed_dev = json.load(f)

with open("canonical_to_all_ids.json") as f:
    canonical_to_all_ids = json.load(f)

dev_loader = DataLoader(dev_dataset, batch_size=32)

id2label = {
    0: "SUPPORTS",
    1: "REFUTES",
    2: "NOT_ENOUGH_INFO",
    3: "DISPUTED"
}

# predictions
predictions = {}
for cid in dev_claims:
    predictions[cid] = {
        "claim_label": None,
        "evidences": []
    }

all_preds = []
with torch.no_grad():
    for batch in tqdm(dev_loader, desc="Generating predictions"):
        inputs = batch['input_ids'].to(device)
        logits = model(inputs)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())

# Write prediction tags
for i, cid in enumerate(dev_claims.keys()):
    canonical_ids = precomputed_dev[cid]["evidence_ids"]
    expanded_ids = expand_ids(canonical_ids, canonical_to_all_ids, top_k=5)

    predictions[cid]["claim_label"] = id2label[all_preds[i]]
    predictions[cid]["evidences"] = expanded_ids

with open("dev-claims-lstm-tfidf.json", "w") as f:
    json.dump(predictions, f, indent=2)

!python3 eval.py --predictions dev-claims-lstm-tfidf.json --groundtruth dev-claims.json

"""# Visualization with the Results

This section visualizes model performance using a **confusion matrix** and **per-class metrics bar chart**. These help identify class-level weaknesses and label biases.

## Confusion Matrix (Classification Breakdown)

Displays how frequently each class is confused with another. Useful for:
- Detecting class-specific errors (e.g., REFUTES misclassified as SUPPORTS)
- Spotting class imbalance or bias
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_true = [dev_claims[cid]['claim_label'] for cid in predictions]
y_pred = [predictions[cid]['claim_label'] for cid in predictions]

label_order = ["SUPPORTS", "REFUTES", "NOT_ENOUGH_INFO", "DISPUTED"]
cm = confusion_matrix(y_true, y_pred, labels=label_order)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_order)
disp.plot(cmap="Blues", xticks_rotation=45)

"""## Per-Class Precision, Recall, and F1 Score

Shows the model's performance per class and highlights imbalanced or weak categories.
"""

import pandas as pd
import matplotlib.pyplot as plt

report = classification_report(y_true, y_pred, labels=label_order, output_dict=True)
df = pd.DataFrame(report).T.loc[label_order, ["precision", "recall", "f1-score"]]

df.plot(kind='bar', figsize=(10, 5))
plt.title("Per-Class Precision, Recall, and F1")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.grid(True, axis='y')