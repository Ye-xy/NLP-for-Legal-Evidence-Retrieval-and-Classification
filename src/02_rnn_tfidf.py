# -*- coding: utf-8 -*-
"""02_RNN_TFIDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_vPWmYcqhzueoMlWSKNWdklLmvk4jrrs

# 2025 COMP90042 Project - Group 43

## Object-Oriented Programming (OOP) Components

This section defines the core classification pipeline using object-oriented programming principles to promote **modularity**, **readability**, and **reuse**.

### Implemented Classes:
- `RNNDatasetCached`: A PyTorch `Dataset` for loading claim-evidence pairs with cached retrieval.
- `RNNClassifier`: A simple RNN-based sequence classifier with an embedding layer.

These components are designed to be easily integrated into training and evaluation loops.
"""

import re
import torch
from torch.utils.data import Dataset
import torch.nn as nn

def tokenize(text):
    return re.findall(r'\w+', text.lower())

class RNNDatasetCached(Dataset):
    def __init__(self, claims_data, evidence_dict, retrieved_evidences, vocab, max_len=128):
        self.samples = list(claims_data.items())
        self.evidence_dict = evidence_dict
        self.retrieved_evidences = retrieved_evidences  # precomputed evidence IDs
        self.vocab = vocab
        self.max_len = max_len
        self.label2id = {"SUPPORTS": 0, "REFUTES": 1, "NOT_ENOUGH_INFO": 2, "DISPUTED": 3}

    def __len__(self):
        return len(self.samples)

    def encode(self, text):
        tokens = tokenize(text)
        token_ids = [self.vocab.get(t, self.vocab['<UNK>']) for t in tokens]
        if len(token_ids) < self.max_len:
            token_ids += [self.vocab['<PAD>']] * (self.max_len - len(token_ids))
        else:
            token_ids = token_ids[:self.max_len]
        return torch.tensor(token_ids)

    def __getitem__(self, idx):
        claim_id, info = self.samples[idx]
        claim = info['claim_text']
        label = self.label2id[info['claim_label']]
        evidences = self.retrieved_evidences[claim_id]
        evidence_texts = [self.evidence_dict.get(eid, '') for eid in evidences]
        input_text = claim + ' ' + ' '.join(evidence_texts)
        input_ids = self.encode(input_text)

        return {
            "claim_id": claim_id,
            "input_ids": input_ids,
            "label": torch.tensor(label, dtype=torch.long)
        }

class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        _, hn = self.rnn(x)
        logits = self.fc(hn.squeeze(0))  # use last hidden state
        return logits

"""# 1.DataSet Processing

## Setup: Load Libraries and Datasets

This section loads all required Python libraries and the datasets needed for training and evaluation.

### Steps:
- Imports necessary libraries (`torch`, `transformers`, `sklearn`, etc.)
- Loads the JSON datasets:
  - `train-claims.json`
  - `dev-claims.json`
  - `evidence.json`
- Initializes tokenizer and random seed
- Applies stratified splitting on training claims
- Deduplicates the evidence corpus based on normalized MD5 hashes
"""

!pip install sentence-transformers

import json
import torch
import hashlib
from collections import defaultdict
from transformers import AutoTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np
import random
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Load the datasets
with open('train-claims.json', 'r') as f:
    train_claims = json.load(f)

with open('dev-claims.json', 'r') as f:
    dev_claims = json.load(f)

with open('evidence.json', 'r') as f:
    evidence_dict = json.load(f)

evidence_texts = list(evidence_dict.values())
evidence_ids = list(evidence_dict.keys())

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Random seed setting
def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
seed_everything()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Prepare claim IDs and their labels
claim_ids = list(train_claims.keys())
labels = [train_claims[cid]["claim_label"] for cid in claim_ids]

# Stratified split (e.g., 80% train, 20% dev)
train_ids, dev_ids = train_test_split(
    claim_ids,
    test_size=0.2,
    stratify=labels,
    random_state=42
)

# Rebuild split claim dictionaries
train_claims_strat = {cid: train_claims[cid] for cid in train_ids}
dev_claims_strat   = {cid: train_claims[cid] for cid in dev_ids}

"""## Evidence Deduplication (by Text Hashing)

Normalizes and hashes each evidence text to identify exact duplicates.
Only one canonical version is kept for retrieval; duplicates are mapped for later expansion if needed.

"""

def normalize_text(text):
    return text.strip().lower()

def hash_text(text):
    return hashlib.md5(normalize_text(text).encode()).hexdigest()

# Build deduplicated evidence dict
hash_to_ids = defaultdict(list)
hash_to_text = {}

for evid_id, text in evidence_dict.items():
    h = hash_text(text)
    hash_to_ids[h].append(evid_id)
    if h not in hash_to_text:
        hash_to_text[h] = text

# Canonical evidence set for indexing
dedup_evidence_dict = {hash_to_ids[h][0]: hash_to_text[h] for h in hash_to_text}
evidence_ids = list(dedup_evidence_dict.keys())
evidence_texts = list(dedup_evidence_dict.values())

# Map from canonical ID to all original duplicates (for later output expansion)
canonical_to_all_ids = {hash_to_ids[h][0]: hash_to_ids[h] for h in hash_to_ids}

"""## TF-IDF Retriever Setup

Fits a TF-IDF vectorizer over the deduplicated evidence corpus and defines a function to retrieve top-K similar passages.

- Uses unigrams only
- Removes English stopwords
- Ignores overly common and rare terms (`max_df=0.9`, `min_df=5`)

"""

# TF-IDF Retriever setup

# Fit TF-IDF vectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)
evidence_tfidf = vectorizer.fit_transform(evidence_texts)

# Define TF-IDF retrieval function
def retrieve_tfidf(claim_text, top_k=5):
    claim_vec = vectorizer.transform([claim_text])
    similarities = cosine_similarity(claim_vec, evidence_tfidf).flatten()
    top_k_indices = similarities.argsort()[-top_k:][::-1]
    return [evidence_ids[i] for i in top_k_indices]

"""## Gold Evidence Injection

This function ensures that gold evidence is present in the retrieved results (e.g., for supervised training). It merges gold IDs with retrieved IDs, preserving order and trimming to `top_k`.

"""

def inject_gold_evidence(train_claims, retrieved_dict, top_k=5):
    for cid, info in train_claims.items():
        gold_ids = info.get("evidences", [])
        current = retrieved_dict.get(cid, [])
        merged = list(dict.fromkeys(gold_ids + current))[:top_k]
        retrieved_dict[cid] = merged

"""# 2.Model Implementation
## RNN Model

This section defines the RNN-based text classification pipeline for claim verification. It includes:
- A vocabulary builder from retrieved evidence
- A standard training loop with early stopping and class weighting

### Vocabulary Construction

Builds a vocabulary from tokenized claimâ€“evidence pairs in the training set.
- Uses top `max_vocab` most frequent words
- Reserves special tokens `<PAD>` and `<UNK>`
"""

from torch.utils.data import Dataset, DataLoader
from collections import Counter
import numpy as np

# Build vocabulary from training data
def build_vocab_with_cache(claims_data, evidence_dict, retrieved_evidences, max_vocab=20000):
    counter = Counter()
    for cid, claim_info in claims_data.items():
        claim = claim_info['claim_text']
        evidences = retrieved_evidences[cid]
        text = claim + ' ' + ' '.join([evidence_dict.get(eid, '') for eid in evidences])
        counter.update(tokenize(text))
    most_common = counter.most_common(max_vocab - 2)
    vocab = {'<PAD>': 0, '<UNK>': 1}
    vocab.update({word: i + 2 for i, (word, _) in enumerate(most_common)})
    return vocab

"""### RNN Training Function

Implements a standard supervised training loop with:
- Weighted cross-entropy loss
- Early stopping based on dev accuracy
- Model checkpoint saving

"""

def train_model(model, train_loader, dev_loader, device, num_epochs=20, patience=5, save_path=None, class_weights=None):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # Use weighted loss if provided
    loss_fn = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()

    best_accuracy = 0
    best_epoch = 0
    patience_counter = 0

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch in train_loader:
            inputs = batch['input_ids'].to(device)
            labels = batch['label'].to(device)

            logits = model(inputs)
            loss = loss_fn(logits, labels)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)

        # Evaluate on dev set
        model.eval()
        correct = total = 0
        with torch.no_grad():
            for batch in dev_loader:
                inputs = batch['input_ids'].to(device)
                labels = batch['label'].to(device)
                logits = model(inputs)
                preds = torch.argmax(logits, dim=1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
        dev_acc = correct / total

        print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Dev Accuracy: {dev_acc:.4f}")

        # Check improvement
        if dev_acc > best_accuracy:
            best_accuracy = dev_acc
            best_epoch = epoch + 1
            patience_counter = 0
            if save_path:
                torch.save(model.state_dict(), save_path)
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping triggered after epoch {epoch+1} (no improvement for {patience} epochs)")
                break

    print(f"\n Best model was at epoch {best_epoch} with dev accuracy = {best_accuracy:.4f}")

"""# 3.Testing and Evaluation
This section evaluates the RNN classifier trained on TF-IDF-retrieved evidence. It includes:

- Classification performance on the dev set
- Training with class imbalance handling
- Prediction file generation with evidence ID expansion
"""

from sklearn.metrics import classification_report

# Evaluation helper
def evaluate_model(model, dev_loader, device):
    model.to(device)
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in dev_loader:
            inputs = batch['input_ids'].to(device)
            labels = batch['label'].to(device)

            logits = model(inputs)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print(classification_report(
        all_labels, all_preds,
        target_names=["SUPPORTS", "REFUTES", "NOT_ENOUGH_INFO", "DISPUTED"]
    ))

"""## TF-IDF Retriever Evaluation

Precompute retrievals, inject gold evidence, build datasets, and train the classifier.

"""

from sklearn.utils.class_weight import compute_class_weight

print("TF-IDF Retriever")

# Precompute retrievals
tfidf_train_retrieved = {
    cid: retrieve_tfidf(info["claim_text"]) for cid, info in train_claims_strat.items()
}

tfidf_dev_retrieved   = {cid: retrieve_tfidf(info["claim_text"], top_k=5) for cid, info in dev_claims_strat.items()}

# Inject gold evidence (during training only)
inject_gold_evidence(train_claims_strat, tfidf_train_retrieved, top_k=5)

# Build vocab + dataset
vocab_tfidf = build_vocab_with_cache(train_claims_strat, evidence_dict, tfidf_train_retrieved)
train_dataset = RNNDatasetCached(train_claims_strat, evidence_dict, tfidf_train_retrieved, vocab_tfidf)
dev_dataset   = RNNDatasetCached(dev_claims_strat, evidence_dict, tfidf_dev_retrieved, vocab_tfidf)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
dev_loader   = DataLoader(dev_dataset, batch_size=32)

# RNN model
model = RNNClassifier(len(vocab_tfidf), 128, 128, 4)
model_path = "rnn_tfidf.pt"

# Train + Save
# Compute class weights from training data
y = [sample["label"].item() for sample in train_dataset]
class_weights = compute_class_weight("balanced", classes=np.array([0,1,2,3]), y=y)
weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)

train_model(model, train_loader, dev_loader, device, num_epochs=20, patience=5, save_path=model_path, class_weights=weights_tensor)

# Evaluate
model.load_state_dict(torch.load(model_path))
evaluate_model(model, dev_loader, device)

"""## Prediction File Generation

Generates final predictions for the full `dev-claims.json` set and expands canonical evidence IDs back to their original form.

"""

# Expand canonical evidence IDs back to all original IDs
def expand_ids(eids, mapping, top_k=5):
    expanded = []
    for eid in eids:
        expanded.extend(mapping.get(eid, [eid]))
    return list(dict.fromkeys(expanded))[:top_k]

# Load the trained model
model = RNNClassifier(len(vocab_tfidf), 128, 128, 4)
model.load_state_dict(torch.load("rnn_tfidf.pt", map_location=device))
model.to(device)
model.eval()

# Prepare DataLoader
dev_loader = DataLoader(dev_dataset, batch_size=32)

# Reverse label mapping
id2label = {0: "SUPPORTS", 1: "REFUTES", 2: "NOT_ENOUGH_INFO", 3: "DISPUTED"}

# Re-run retrieval on full dev set using deduplicated evidence
tfidf_dev_retrieved_full = {
    cid: retrieve_tfidf(info["claim_text"], top_k=5) for cid, info in dev_claims.items()
}

# Build new dataset using retrieved evidence
full_dev_dataset = RNNDatasetCached(dev_claims, evidence_dict, tfidf_dev_retrieved_full, vocab_tfidf)
full_dev_loader = DataLoader(full_dev_dataset, batch_size=32)

# Prepare predictions dict
predictions = {}

# Generate predictions with evidence expansion
with torch.no_grad():
    for batch in tqdm(full_dev_loader, desc="Generating predictions"):
        inputs = batch['input_ids'].to(device)
        logits = model(inputs)
        preds = torch.argmax(logits, dim=1)
        claim_ids = batch['claim_id']

        for cid, pred in zip(claim_ids, preds.cpu().numpy()):
            expanded_evidences = expand_ids(tfidf_dev_retrieved_full[cid], canonical_to_all_ids, top_k=5)
            predictions[cid] = {
                "claim_label": id2label[pred],
                "evidences": expanded_evidences
            }

# Save to file
with open("dev-claims-rnn-tfidf.json", "w") as f:
    json.dump(predictions, f, indent=2)

!python3 eval.py --predictions dev-claims-rnn-tfidf.json --groundtruth dev-claims.json

"""# Visualization with the Results

This section visualizes model performance using a **confusion matrix** and **per-class metrics bar chart**. These help identify class-level weaknesses and label biases.

## Confusion Matrix (Classification Breakdown)

Displays how frequently each class is confused with another. Useful for:
- Detecting class-specific errors (e.g., REFUTES misclassified as SUPPORTS)
- Spotting class imbalance or bias
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_true = [dev_claims[cid]['claim_label'] for cid in predictions]
y_pred = [predictions[cid]['claim_label'] for cid in predictions]

label_order = ["SUPPORTS", "REFUTES", "NOT_ENOUGH_INFO", "DISPUTED"]
cm = confusion_matrix(y_true, y_pred, labels=label_order)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_order)
disp.plot(cmap="Blues", xticks_rotation=45)

"""## Per-Class Precision, Recall, and F1 Score

Shows the model's performance per class and highlights imbalanced or weak categories.

"""

import pandas as pd
import matplotlib.pyplot as plt

report = classification_report(y_true, y_pred, labels=label_order, output_dict=True)
df = pd.DataFrame(report).T.loc[label_order, ["precision", "recall", "f1-score"]]

df.plot(kind='bar', figsize=(10, 5))
plt.title("Per-Class Precision, Recall, and F1")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.grid(True, axis='y')