# -*- coding: utf-8 -*-
"""01_Exploratory_Retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a0xiJy_GsyrLZPzPWuOVjaC003IcxDeZ

# 01_Exploratory_Retrieval.ipynb

This notebook benchmarks multiple evidence retrieval strategies for claim verification using a climate fact-checking dataset. Each retriever is evaluated based on its ability to retrieve relevant evidence passages that support or refute a given claim.

### Evaluation Metrics

- **Top-5 Hit Rate**  
  Indicates whether at least one gold evidence appears among the top-5 retrieved passages.  
  - *Why it matters:* Ensures the classifier has access to relevant evidence for decision-making.  
  - *Interpretation:* Higher is better.


**Retrievers evaluated:**
- TF-IDF (symbolic)
- BM25 (symbolic)
- FAISS with MiniLM-L6 and MiniLM-L12 embeddings (semantic)
- TF-IDF + FAISS hybrid (interpolated)
- Flan-T5 LLM-based selector (prompt-based)

## 1. Setup and Data Loading

- Loads training/dev claims and evidence corpus from JSON
- Deduplicates evidence by hashing normalized text
- Trims data for efficiency in exploratory runs
"""

!pip install rank_bm25 faiss-cpu sentence-transformers

import json, time, psutil, os, numpy as np, faiss, torch, hashlib
from collections import defaultdict
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
import pandas as pd
from tqdm import tqdm

# Load datasets
with open('train-claims.json') as f: train_claims = json.load(f)
with open('dev-claims.json') as f: dev_claims = json.load(f)
with open('evidence.json') as f: evidence_dict = json.load(f)

# Deduplication
def normalize_text(text): return text.strip().lower()
def hash_text(text): return hashlib.md5(normalize_text(text).encode()).hexdigest()

hash_to_ids, hash_to_text = defaultdict(list), {}
for evid_id, text in evidence_dict.items():
    h = hash_text(text)
    hash_to_ids[h].append(evid_id)
    if h not in hash_to_text: hash_to_text[h] = text

dedup_evidence_dict = {hash_to_ids[h][0]: hash_to_text[h] for h in hash_to_text}
canonical_to_all_ids = {hash_to_ids[h][0]: hash_to_ids[h] for h in hash_to_ids}

# Trim for speed
dev_claims = dict(list(dev_claims.items())[:50000])
evidence_texts = list(dedup_evidence_dict.values())[:200000]
evidence_ids = list(dedup_evidence_dict.keys())[:200000]

"""## 2. Evaluation Metrics

Implements Top-5 Hit Rate for evaluating retrieval quality.

"""

top_k = 5
process = psutil.Process()

def evaluate_topk_hit_rate(claims, fn, top_k=5):
    return round(sum(any(e in set(info["evidences"]) for e in fn(info["claim_text"], top_k))
                     for info in claims.values()) / len(claims), 4)

def evaluate_retriever(name, fn, claims, init_time=None, ram_used=None, top_k=5):
    return {
        "Model": name,
        "Top-5 Hit Rate": evaluate_topk_hit_rate(claims, fn, top_k),
        "Init Time (s)": "-" if init_time is None else round(init_time, 2),
        "RAM Used (MB)": "-" if ram_used is None else round(ram_used, 2),
    }

"""## 3. Symbolic Retrievers: TF-IDF and BM25

"""

# TF-IDF Retriever setup

start_time = time.time()
start_mem = process.memory_info().rss / 1024 / 1024

vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)
tfidf_matrix = vectorizer.fit_transform(evidence_texts)

def tfidf_retrieve_fn(text, top_k=5):
    vec = vectorizer.transform([text])
    sims = cosine_similarity(vec, tfidf_matrix).flatten()
    idx = sims.argsort()[-top_k:][::-1]
    return [evidence_ids[i] for i in idx]

tfidf_stats = evaluate_retriever("TF-IDF", tfidf_retrieve_fn, dev_claims,
                                  time.time() - start_time,
                                  process.memory_info().rss / 1024 / 1024 - start_mem)

# BM25 Retriever setup

start_time = time.time()
start_mem = process.memory_info().rss / 1024 / 1024

tokenized = [text.lower().split() for text in evidence_texts]
bm25 = BM25Okapi(tokenized)

def bm25_retrieve_fn(text, top_k=5):
    tokens = text.lower().split()
    scores = bm25.get_scores(tokens)
    idx = np.argsort(scores)[-top_k:][::-1]
    return [evidence_ids[i] for i in idx]

bm25_stats = evaluate_retriever("BM25", bm25_retrieve_fn, dev_claims,
                                 time.time() - start_time,
                                 process.memory_info().rss / 1024 / 1024 - start_mem)

"""## 4. Semantic Retrievers: FAISS with MiniLM Embeddings

"""

# FAISS-L6 Setup
start_time = time.time()
start_mem = process.memory_info().rss / 1024 / 1024

embedder_l6 = SentenceTransformer("all-MiniLM-L6-v2")
evecs_l6 = embedder_l6.encode(evidence_texts, convert_to_numpy=True).astype("float32")
evecs_l6 = normalize(evecs_l6, axis=1)
faissl6_index = faiss.IndexFlatIP(evecs_l6.shape[1])
faissl6_index.add(evecs_l6)

def faiss_retrieve_l6(text, top_k=5):
    vec = embedder_l6.encode([text], convert_to_numpy=True).astype("float32")
    vec = normalize(vec, axis=1)
    D, I = faissl6_index.search(vec, top_k)
    return [evidence_ids[i] for i in I[0]]

faiss_l6_stats = evaluate_retriever("FAISS (L6)", faiss_retrieve_l6, dev_claims,
                                    time.time() - start_time,
                                    process.memory_info().rss / 1024 / 1024 - start_mem)

# FAISS-L12 Setup
start_time = time.time()
start_mem = process.memory_info().rss / 1024 / 1024

embedder_l12 = SentenceTransformer("all-MiniLM-L12-v2")
evecs_l12 = embedder_l12.encode(evidence_texts, convert_to_numpy=True).astype("float32")
evecs_l12 = normalize(evecs_l12, axis=1)
faissl12_index = faiss.IndexFlatIP(evecs_l12.shape[1])
faissl12_index.add(evecs_l12)

def faiss_retrieve_l12(text, top_k=5):
    vec = embedder_l12.encode([text], convert_to_numpy=True).astype("float32")
    vec = normalize(vec, axis=1)
    D, I = faissl12_index.search(vec, top_k)
    return [evidence_ids[i] for i in I[0]]

faiss_l12_stats = evaluate_retriever("FAISS (L12)", faiss_retrieve_l12, dev_claims,
                                     time.time() - start_time,
                                     process.memory_info().rss / 1024 / 1024 - start_mem)

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto")

def build_llm_prompt(claim, candidates, k):
    prompt = f"Claim: {claim}\n"
    prompt += "Sentences:\n" + "\n".join([f"{i+1}. {c}" for i, c in enumerate(candidates)])
    prompt += f"\nList the top {k} most relevant sentence numbers, e.g., 1, 2, 3, 4, 5"
    return prompt

def llm_fn(text, top_k=5):
    prompt = build_llm_prompt(text, evidence_texts[:100], top_k)
    input_ids = tokenizer(prompt[:3000], return_tensors="pt", truncation=True).input_ids.to(device)
    outputs = model.generate(input_ids, max_new_tokens=100)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    try:
        selected = [int(x.strip()) - 1 for x in response.split(",")]
        return [evidence_ids[i] for i in selected if 0 <= i < len(evidence_ids)]
    except:
        return []

llm_stats = evaluate_retriever("LLM Retriever", llm_fn, dev_claims)

"""## 5. Hybrid Retriever: TF-IDF + FAISS Interpolation

"""

# Hybrid Retriever

def run_hybrid_retriever(alpha, tfidf_matrix, vectorizer, faiss_index, embedder, evidence_ids):
    def retrieve_fn(text, top_k=5):
        tfidf_vec = vectorizer.transform([text])
        tfidf_scores = cosine_similarity(tfidf_vec, tfidf_matrix).flatten()

        faiss_vec = embedder.encode([text], convert_to_numpy=True).astype("float32")
        faiss_vec = normalize(faiss_vec, axis=1)
        D, I = faiss_index.search(faiss_vec, len(evidence_ids))
        faiss_scores = np.zeros(len(evidence_ids))
        faiss_scores[I[0]] = D[0]

        tfidf_norm = (tfidf_scores - np.min(tfidf_scores)) / (np.ptp(tfidf_scores) + 1e-8)
        faiss_norm = (faiss_scores - np.min(faiss_scores)) / (np.ptp(faiss_scores) + 1e-8)

        hybrid = alpha * tfidf_norm + (1 - alpha) * faiss_norm
        idx = hybrid.argsort()[-top_k:][::-1]
        return [evidence_ids[i] for i in idx]
    return retrieve_fn

retriever_stats = {
    "TF-IDF": tfidf_stats,
    "BM25": bm25_stats,
    "FAISS (L6)": faiss_l6_stats,
    "FAISS (L12)": faiss_l12_stats,
    "LLM Retriever": llm_stats
}

# Hybrid setup
best_alpha = None
best_score = -1
best_stats = None

# Evaluate hybrids (alpha hyperparameter tuning)
for alpha in [0.05, 0.1, 0.2, 0.3, 0.4]:
    key = f"Hybrid TF-IDF+FAISS (α={alpha})"

    retrieve_fn = run_hybrid_retriever(
        alpha=alpha,
        tfidf_matrix=tfidf_matrix,
        faiss_index=faissl12_index,
        embedder=embedder_l12,
        evidence_ids=evidence_ids,
        vectorizer=vectorizer
    )

    stats = evaluate_retriever(
        name=key,
        fn=retrieve_fn,
        claims=dev_claims,
        init_time=None,
        ram_used=None,
        top_k=5
    )

    retriever_stats[key] = stats

    if stats["Top-5 Hit Rate"] > best_score:
        best_score = stats["Top-5 Hit Rate"]
        best_alpha = alpha
        best_stats = stats

retriever_stats[f"Hybrid (best α={best_alpha})"] = best_stats

from IPython.display import display
import pandas as pd

# Create full results DataFrame
df = pd.DataFrame(retriever_stats).T

# Metrics to highlight (higher is better)
highlight_cols = ["Top-5 Hit Rate"]

# Format specification for numeric columns
format_dict = {
    "Top-5 Hit Rate": "{:.4f}",
    "Init Time (s)": "{:.2f}",
    "RAM Used (MB)": "{:.2f}"
}

# Coerce numeric columns (skip "-" values)
for col in highlight_cols:
    df[col] = pd.to_numeric(df[col], errors="coerce")

# Safe formatting wrapper
def safe_format(val, fmt):
    try:
        return fmt.format(val)
    except (ValueError, TypeError):
        return val

# Apply safe formatting per column
safe_format_dict = {col: (lambda fmt=fmt: lambda x: safe_format(x, fmt))() for col, fmt in format_dict.items()}

# Style table
styled_df = (
    df.style.set_caption("Retrieval Benchmark Summary")
    .highlight_max(subset=highlight_cols, color="lightgreen")
    .format(safe_format_dict)
)

display(styled_df)