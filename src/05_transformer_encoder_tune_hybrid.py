# -*- coding: utf-8 -*-
"""05_Transformer_Encoder_Tune_Hybrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLVkXDRMffn9CBso8_yaSiBSC4LwaZUf

# 2025 COMP90042 Project - Group 43

## Readme
Due to the lengh of the transformer implementation and clarity, we put the transformer related OOP component under the Model Implementation section.

## Object-Oriented Programming (OOP) Components

This section defines the core classification pipeline using object-oriented programming principles to promote **modularity**, **readability**, and **reuse**.

### Implemented Classes:
- `FactCheckingDataset`: A PyTorch `Dataset` for loading claim-evidence pairs with retrieval.
It also ensures that gold evidence is present in the retrieved results (e.g., for supervised training). It merges gold IDs with retrieved IDs, preserving order and trimming to `top_k`.
"""

import torch
from torch.utils.data import Dataset

class FactCheckingDataset(Dataset):
    def __init__(self, claims_data, evidence_dict, retriever_fn, tokenizer, max_length=128, top_k=5, inject=False):
        self.label2id = {"SUPPORTS": 0, "REFUTES": 1, "NOT_ENOUGH_INFO": 2, "DISPUTED": 3}
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []

        for claim_id, claim_info in claims_data.items():
            claim_text = claim_info['claim_text']
            label_text = claim_info['claim_label']
            label = self.label2id[label_text]

            gold_ids = claim_info.get("evidences", [])
            retrieved_ids = retriever_fn(claim_text, top_k=top_k)

            if inject:
                retrieved_ids = list(dict.fromkeys(gold_ids + retrieved_ids))[:top_k]

            evidence_texts = [evidence_dict.get(eid, "") for eid in retrieved_ids]
            input_text = claim_text + " [SEP] " + " [SEP] ".join(evidence_texts)

            inputs = tokenizer(
                input_text,
                truncation=True,
                padding="max_length",
                max_length=max_length,
                return_tensors="pt"
            )

            self.data.append({
                "claim_id": claim_id,
                "claim_text": claim_text,
                "input_ids": inputs["input_ids"].squeeze(0),
                "attention_mask": inputs["attention_mask"].squeeze(0),
                "label": torch.tensor(label, dtype=torch.long),
                "raw_text": input_text,
                "evidences": retrieved_ids,
                "gold_ids": gold_ids
            })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

"""# 1.DataSet Processing

## Setup: Load Libraries and Datasets

This section loads all required Python libraries and the datasets needed for training and evaluation.

### Steps:
- Imports necessary libraries (`torch`, `transformers`, `sklearn`, etc.)
- Loads the JSON datasets:
  - `train-claims.json`
  - `dev-claims.json`
  - `evidence.json`
- Initializes tokenizer and random seed
- Applies stratified splitting on training claims
- Deduplicates the evidence corpus based on normalized MD5 hashes
"""

!pip install faiss-cpu sentence-transformers

import json
import torch
from transformers import AutoTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import random
from sklearn.model_selection import train_test_split

# Load the datasets
with open('train-claims.json', 'r') as f:
    train_claims = json.load(f)

with open('dev-claims.json', 'r') as f:
    dev_claims = json.load(f)

with open('evidence.json', 'r') as f:
    evidence_dict = json.load(f)

evidence_texts = list(evidence_dict.values())
evidence_ids = list(evidence_dict.keys())

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Random seed setting
def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
seed_everything()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Prepare claim IDs and their labels
claim_ids = list(train_claims.keys())
labels = [train_claims[cid]["claim_label"] for cid in claim_ids]

# Sepetate data for trainging and evaluation
train_ids, dev_ids = train_test_split(
    claim_ids,
    test_size=0.2,
    stratify=labels,
    random_state=42
)

# Get claim based on the split
train_claims_strat = {cid: train_claims[cid] for cid in train_ids}
dev_claims_strat   = {cid: train_claims[cid] for cid in dev_ids}

"""## Evidence Deduplication (by Text Hashing)

Normalizes and hashes each evidence text to identify exact duplicates.
Only one canonical version is kept for retrieval; duplicates are mapped for later expansion if needed.
"""

from sklearn.preprocessing import normalize
from collections import defaultdict
import hashlib

def normalize_text(text):
    return text.strip().lower()

def hash_text(text):
    return hashlib.md5(normalize_text(text).encode()).hexdigest()

# Deduplicate
hash_to_ids = defaultdict(list)
hash_to_text = {}

for evid_id, text in evidence_dict.items():
    h = hash_text(text)
    hash_to_ids[h].append(evid_id)
    if h not in hash_to_text:
        hash_to_text[h] = text

# Canonical evidence set for indexing
dedup_evidence_dict = {hash_to_ids[h][0]: hash_to_text[h] for h in hash_to_text}
evidence_ids = list(dedup_evidence_dict.keys())
evidence_texts = list(dedup_evidence_dict.values())

# Map from canonical ID to all original duplicates (for later output expansion)
canonical_to_all_ids = {hash_to_ids[h][0]: hash_to_ids[h] for h in hash_to_ids}

"""## Setup for Each Retriever Methods  
- `retrieve_tfidf()`: Symbolic retrieval using TF-IDF similarity  
- `retrieve_faiss()`: Semantic retrieval using dense MiniLM embeddings + FAISS  
- `make_hybrid_retriever()`: Interpolated retriever that combines TF-IDF and FAISS scores


"""

# TF-IDF Retriever setup

# Improved TF-IDF vectorizer: ignore very common or very rare terms
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)
evidence_tfidf = vectorizer.fit_transform(evidence_texts)

# Define TF-IDF retrieval function
def retrieve_tfidf(claim_text, top_k=5):
    claim_vec = vectorizer.transform([claim_text])
    similarities = cosine_similarity(claim_vec, evidence_tfidf).flatten()
    top_k_indices = similarities.argsort()[-top_k:][::-1]
    return [evidence_ids[i] for i in top_k_indices]

# FAISS Retriever setup

# Configs
top_k = 5
model_name = "all-MiniLM-L12-v2"

# Load embedding model
embedder = SentenceTransformer(model_name, device=device)

# Encode and normalize evidence corpus
evecs = embedder.encode(evidence_texts, convert_to_numpy=True).astype("float32")
evecs = normalize(evecs, axis=1)  # normalize for cosine similarity

# Build FAISS index with cosine similarity (dot product after normalization)
faiss_index = faiss.IndexFlatIP(evecs.shape[1])
faiss_index.add(evecs)

# Define FAISS retrieval function
def retrieve_faiss(claim_text, top_k=5):
    query = embedder.encode([claim_text], convert_to_numpy=True).astype("float32")
    query = normalize(query, axis=1)  # normalize query vector
    D, I = faiss_index.search(query, top_k)
    return [evidence_ids[i] for i in I[0]]

# Hybrid Retriever setup
# with TF-IDF and FAISS

def make_hybrid_retriever(alpha, vectorizer, evidence_tfidf, faiss_index, embedder, evidence_ids):
    def retrieve_fn(claim_text, top_k=5):

        query_vec = vectorizer.transform([claim_text])
        tfidf_scores = (query_vec @ evidence_tfidf.T).toarray()[0]

        # FAISS score
        faiss_vec = embedder.encode([claim_text], convert_to_numpy=True).astype("float32")
        D, I = faiss_index.search(faiss_vec, len(evidence_ids))
        faiss_scores = np.zeros(len(evidence_ids))
        faiss_scores[I[0]] = D[0]

        # Normalize
        tfidf_norm = (tfidf_scores - tfidf_scores.min()) / (np.ptp(tfidf_scores) + 1e-8)
        faiss_norm = (faiss_scores - faiss_scores.min()) / (np.ptp(faiss_scores) + 1e-8)

        # Fuse
        hybrid = alpha * tfidf_norm + (1 - alpha) * faiss_norm
        top_idx = hybrid.argsort()[-top_k:][::-1]
        return [evidence_ids[i] for i in top_idx]
    return retrieve_fn

"""# 2.Model Implementation
## Transformer Encoder Model

This section defines the transformer-based text classification pipeline for claim verification.

"""

import math
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F
import copy

class Embeddings(nn.Module):
    def __init__(self, d_model, vocab_size):
        super(Embeddings, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        return self.dropout(x)

class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0

        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]

        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)

        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)

class Encoder(nn.Module):
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)

    def forward(self, x, src_mask):
        for layer in self.layers:
            x = layer(x, src_mask)
        return x

class EncoderLayer(nn.Module):
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)

    def forward(self, x, src_mask):
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, src_mask))
        return self.sublayer[1](x, self.feed_forward)

class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

class SublayerConnection(nn.Module):
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return self.dropout(self.norm(x + sublayer(x)))

class TransformerEncoderFactAnalyzer(nn.Module):
  def __init__(self, vocab_size, N=4, d_model=32, d_ff=64, h=2, dropout=0.1):
    super(TransformerEncoderFactAnalyzer, self).__init__()

    c = copy.deepcopy

    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)

    self.embed = nn.Sequential(Embeddings(d_model, vocab_size), c(position))
    self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)
    self.fc = nn.Linear(d_model, 4)

  def forward(self, input):
    src = input[0]
    padding_mask = input[1]

    embedding = self.embed(src)
    encoder_output = self.encoder(embedding, padding_mask.unsqueeze(1))

    last_indices = padding_mask.sum(dim=1) - 1
    batch_indices = torch.arange(encoder_output.size(0)).to(encoder_output.device)
    selected_outputs = encoder_output[batch_indices, last_indices, :]

    # [batch_size, n_classes]
    logits = self.fc(selected_outputs)
    return logits

def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

def clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

def transformer_tokenizer(x, vocab):
    tokens = tokenizer(x)
    return [vocab.get(token, default_index) for token in tokens]

def transformer_seq_collate_batch(batch):
    max_len = 20
    label_list, text_list, id_list = [], [], []
    for _text, _label, _id in batch:
        label_list.append(_label)
        text_list.append(transformer_tokenizer(_text, vocab))
        id_list.append(_id)
    label_list = torch.tensor(label_list, dtype=torch.long)

    padded_sequences = []
    attention_masks = []
    for seq in text_list:
        truncated_seq = seq[:max_len]
        padded_seq = truncated_seq + [padding_index] * (max_len - len(truncated_seq))
        padded_sequences.append(torch.tensor(padded_seq))
        mask = [1 if token != padding_index else 0 for token in padded_seq]
        attention_masks.append(torch.tensor(mask))

    text_tensor = torch.stack(padded_sequences)
    mask_tensor = torch.stack(attention_masks)

    return (text_tensor.to(device), mask_tensor.to(device)), label_list.to(device), id_list

def yield_tokens(data_iter):
    for text in data_iter:
        yield tokenizer(text)

def build_vocab_from_iterator(token_iterator, specials=('<CLS>', '<UNK>', '<PAD>', '<SEP>')):
    counter = Counter()
    for tokens in token_iterator:
        counter.update(tokens)

    vocab = {token: idx + len(specials) for idx, (token, _) in enumerate(counter.items())}
    for idx, special in enumerate(specials):
        vocab[special] = idx

    stoi = vocab
    itos = {idx: token for token, idx in stoi.items()}

    return stoi, itos, stoi['<UNK>']

"""### Transformer Training Function

Implements a standard supervised training loop with:
- Weighted cross-entropy loss

"""

def train(dataloader, model, loss_fn, optimizer, save_path=False):
    model.train()
    for batch_idx, ((X, attention_mask), y, _) in enumerate(dataloader):
        pred = model((X, attention_mask))
        loss = loss_fn(pred, y)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch_idx == len(dataloader) - 1:
            print(f"loss: {loss.item():>7f}")

    if save_path:
        torch.save(model.state_dict(), save_path)

"""# 3.Testing and Evaluation
This section evaluates the transformer classifier trained on Hybrid-retrieved evidence. It includes:

- Classification performance on the dev set
- Training with class imbalance handling
- Prediction file generation with evidence ID expansion
"""

from sklearn.metrics import classification_report

def evaluate_model(dataloader, model, loss_fn=None):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for (X, attention_mask), y, _ in dataloader:
            outputs = model((X, attention_mask))
            preds = torch.argmax(outputs, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    print(classification_report(
        all_labels, all_preds, target_names=["SUPPORTS", "REFUTES", "NOT_ENOUGH_INFO", "DISPUTED"]
    ))

from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter
from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader

# Build hybrid retriever
retrieve_hybrid = make_hybrid_retriever(0.2, vectorizer, evidence_tfidf, faiss_index, embedder, evidence_ids)

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Create train dataset
train_dataset = FactCheckingDataset(
    claims_data=train_claims_strat,
    evidence_dict=evidence_dict,
    retriever_fn=retrieve_hybrid,
    tokenizer=tokenizer,
    max_length=128,
    top_k=5,
    inject=True
)

train_dev_dataset = FactCheckingDataset(
    claims_data=dev_claims_strat,
    evidence_dict=evidence_dict,
    retriever_fn=retrieve_hybrid,
    tokenizer=tokenizer,
    max_length=128,
    top_k=5,
    inject=False
)

train_x = [train_dataset[i]['raw_text'] for i in range(len(train_dataset))]
train_y = [train_dataset[i]['label'] for i in range(len(train_dataset))]
train_ids = [train_dataset[i]['claim_id'] for i in range(len(train_dataset))]

train_dev_x = [train_dev_dataset[i]['raw_text'] for i in range(len(train_dev_dataset))]
train_dev_y = [train_dev_dataset[i]['label'] for i in range(len(train_dev_dataset))]
train_dev_ids = [train_dev_dataset[i]['claim_id'] for i in range(len(train_dev_dataset))]

# Build the vocabulary
vocab, index_to_vocab, default_index = build_vocab_from_iterator(yield_tokens(train_x), specials=('<CLS>', '<UNK>', '<PAD>', '<SEP>'))
padding_index = vocab['<PAD>']

counter_vectorizer = CountVectorizer(
    tokenizer=tokenizer,
    vocabulary=vocab,
    lowercase=True
)

xseq_train_dataloader = DataLoader(
    list(zip(train_x, train_y, train_ids)),
    batch_size=10,
    collate_fn=transformer_seq_collate_batch
)
xseq_train_dev_dataloader = DataLoader(
    list(zip(train_dev_x, train_dev_y, train_dev_ids)),
    batch_size=10,
    collate_fn=transformer_seq_collate_batch
)

"""### Hyper parameter tuning
This section evaluates different possible hyper parameter.A 15 random combination should be selected and tested. A best one with the lowest loss would be selected for the down stream tasks.
"""

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)

    model.eval()
    test_loss, correct = 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for (X, attention_mask), y, _ in dataloader:
            X, attention_mask, y = X.to(device), attention_mask.to(device), y.to(device)

            pred = model((X, attention_mask))
            loss = loss_fn(pred, y)
            test_loss += loss.item()

            predicted_class = pred.argmax(dim=1)
            correct += (predicted_class == y).sum().item()

            all_preds.extend(predicted_class.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    test_loss /= num_batches
    correct /= size

    return correct, test_loss

from sklearn.model_selection import ParameterGrid
from sklearn.model_selection import ParameterSampler
import copy

vocab_size = len(vocab)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# tune hyper parameter
param = {
    'num_layers': [2, 10, 20, 30],
    'd_model': [32, 64, 128, 256],
    'd_ff': [64, 128],
    'h': [2, 16, 32, 64],
    'dropout': [0.1, 0.3, 0.5],
    'lr': [0.0001, 0.001]
}

n_iter = 15
sampler = list(ParameterSampler(param, n_iter=n_iter, random_state=42))
print(f"Sampling {n_iter} combinations out of {np.prod([len(v) for v in param.values()])} total.")

best_dev_loss = float('inf')
best_param = None

for i, param in enumerate(sampler):
  if param['d_model'] % param['h'] != 0:
    continue

  print(f"Param sample: { i }")
  transformer_model = TransformerEncoderFactAnalyzer(vocab_size, param['num_layers'], d_model=param['d_model'], d_ff=param['d_ff'], h=param['h'], dropout=param['dropout']).to(device)
  loss_fn = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(transformer_model.parameters(), lr=param['lr'])

  epochs = 3
  for t in range(epochs):
    print(f"Epoch {t + 1}\n-------------------------------")
    train(xseq_train_dataloader, transformer_model, loss_fn, optimizer)

  correct, loss = test(xseq_train_dev_dataloader, transformer_model, loss_fn)
  if loss < best_dev_loss:
    best_dev_loss = loss
    best_param = param

print("Best param:")
print(best_param)

transformer_hyper_model = TransformerEncoderFactAnalyzer(len(vocab), best_param['num_layers'], d_model=best_param['d_model'], d_ff=best_param['d_ff'], h=best_param['h'], dropout=best_param['dropout']).to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(transformer_hyper_model.parameters(), lr=best_param['lr'])

model_path = "transformer_hybrid_hyper.pt"
epochs = 20
for t in range(epochs):
  print(f"Epoch {t + 1}\n-------------------------------")
  train(xseq_train_dataloader, transformer_hyper_model, loss_fn, optimizer, save_path=model_path)

transformer_hyper_model.load_state_dict(torch.load(model_path))
evaluate_model(xseq_train_dev_dataloader, transformer_hyper_model, loss_fn)

"""## Prediction File Generation

Generates final predictions for the full `dev-claims.json` set and expands canonical evidence IDs back to their original form.

"""

# Expand canonical evidence IDs back to all original IDs
def expand_ids(eids, mapping, top_k=5):
    expanded = []
    for eid in eids:
        expanded.extend(mapping.get(eid, [eid]))
    return list(dict.fromkeys(expanded))[:top_k]

from tqdm import tqdm

model = transformer_hyper_model

# Create dev dataset
dev_dataset = FactCheckingDataset(
    claims_data=dev_claims,
    evidence_dict=evidence_dict,
    retriever_fn=retrieve_hybrid,
    tokenizer=tokenizer,
    max_length=128,
    top_k=5
)

dev_x = [dev_dataset[i]['raw_text'] for i in range(len(dev_dataset))]
dev_y = [dev_dataset[i]['label'] for i in range(len(dev_dataset))]
dev_ids = [dev_dataset[i]['claim_id'] for i in range(len(dev_dataset))]

xseq_dev_dataloader = DataLoader(
    list(zip(dev_x, dev_y, dev_ids)),
    batch_size=10,
    collate_fn=transformer_seq_collate_batch
)

claims = {sample["claim_id"]: sample for sample in dev_dataset}

id2label = {0: "SUPPORTS", 1: "REFUTES", 2: "NOT_ENOUGH_INFO", 3: "DISPUTED"}

predictions = {}
model.eval()
with torch.no_grad():
    for (inputs, masks), _, claim_ids in tqdm(xseq_dev_dataloader, desc="Generating predictions"):
        logits = model((inputs, masks))
        preds = torch.argmax(logits, dim=1)

        for cid, pred in zip(claim_ids, preds.cpu().numpy()):
            sample = claims[cid]
            predictions[cid] = {
                "claim_text": sample["claim_text"],
                "claim_label": id2label[pred],
                "evidences": expand_ids(sample['evidences'], canonical_to_all_ids, top_k=5)
            }

# Save to file
with open("dev-claims-transformer-hybrid.json", "w") as f:
    json.dump(predictions, f, indent=2)

!python3 eval.py --predictions dev-claims-transformer-hybrid.json --groundtruth dev-claims.json

"""# Visualization with the Results

This section visualizes model performance using a **confusion matrix**, **per-class metrics bar chart** and a **Evidence selected frequency bar char**. These help identify class-level weaknesses, label biases and the protential issues with the selected evidences.
"""

from collections import Counter
import matplotlib.pyplot as plt

label_names = [id2label[int(l)] for l in train_y]
label_counts = Counter(label_names)

plt.figure(figsize=(6, 4))
plt.bar(label_counts.keys(), label_counts.values())
plt.title("Training Label Distribution")
plt.xlabel("Label")
plt.ylabel("Number of records")
plt.show()

import matplotlib.pyplot as plt
from collections import Counter

all_evidence = []
for i in range(len(dev_dataset)):
    all_evidence.extend(dev_dataset[i]["evidences"])

evidence_counts = Counter(all_evidence)
sorted_counts = sorted(evidence_counts.values(), reverse=True)

plt.figure(figsize=(8, 4))
plt.plot(sorted_counts)
plt.title("Evidence ID Frequency Across Retrieved Results")
plt.xlabel("Evidence Frequency")
plt.ylabel("Number of times retrieved")
plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report

epochs = 20

final_model =TransformerEncoderFactAnalyzer(vocab_size, best_param['num_layers'], d_model=best_param['d_model'], d_ff=best_param['d_ff'], h=best_param['h'], dropout=best_param['dropout']).to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(transformer_model.parameters(), lr=best_param['lr'])
for t in range(epochs):
  print(f"Epoch {t + 1}\n-------------------------------")
  train(xseq_train_dataloader, final_model, loss_fn, optimizer)

test(xseq_dev_dataloader, final_model, loss_fn)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_true = [dev_claims[cid]['claim_label'] for cid in predictions]
y_pred = [predictions[cid]['claim_label'] for cid in predictions]

label_title = ["SUPPORTS", "REFUTES", "NOT_ENOUGH_INFO", "DISPUTED"]
cm = confusion_matrix(y_true, y_pred, labels=label_title)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_title)
disp.plot(cmap="Blues", xticks_rotation=45)

import pandas as pd
import matplotlib.pyplot as plt

report = classification_report(y_true, y_pred, labels=label_title, output_dict=True)
df = pd.DataFrame(report).T.loc[label_title, ["precision", "recall", "f1-score"]]

df.plot(kind='bar', figsize=(10, 5))
plt.title("Per-Class Precision, Recall, and F1")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.grid(True, axis='y')